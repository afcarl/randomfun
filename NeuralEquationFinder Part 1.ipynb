{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import gradcheck\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Let's say y = f(x_1, x_2, ..., x_n)\n",
    "If f is a mathematical function of x_1, x_2, x_3, ..., x_n then we can wire up a neural network to try to find this function given some examples of (x_1, x_2, ...., x_n, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st: Normal Equation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W :  [ -1.49279111e-14   2.00000000e+00   2.43350714e-15  -1.04209881e-16]\n",
      "Predicted Y :  [  0.2   2.    4.    6.   -8.   50. ]\n",
      "RMS loss :  1.1242258769e-14\n"
     ]
    }
   ],
   "source": [
    "# Let's try to find the equation y = 2 * x \n",
    "# We have 6 examples:- (x,y) = (0.1,0.2), (1,2), (2, 4), (3, 6), (-4, -8), (25, 50)\n",
    "# Let's assume y is a linear combination of the features x, x^2, x^3\n",
    "# We know that Normal Equation gives us the exact solution so let's first use that\n",
    "\n",
    "N = 6\n",
    "x = np.array([0.1, 1, 2, 3, -4, 25])\n",
    "y = np.array([0.2, 2, 4, 6, -8, 50])\n",
    "x_2 = x**2\n",
    "x_3 = x**3\n",
    "\n",
    "X = np.ones((N, 4))\n",
    "X[:,1] = x\n",
    "X[:,2] = x_2\n",
    "X[:,3] = x_3\n",
    "\n",
    "_, D = np.shape(X)\n",
    "regularization_strength = 0.0\n",
    "XtX = (X.T).dot(X)\n",
    "I = np.eye(D, dtype=float)\n",
    "XtX_RI = XtX + regularization_strength*I\n",
    "XtY = (X.T).dot(y)\n",
    "w = np.linalg.solve(XtX_RI, XtY)\n",
    "y_pred = X.dot(w)\n",
    "loss = np.sqrt(np.mean((y_pred-y)**2))\n",
    "# As expected w ~ [0 2 0 0]\n",
    "print(\"W : \", w)\n",
    "print(\"Predicted Y : \", y_pred)\n",
    "print(\"RMS loss : \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd: Neural Network Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and test tensors\n",
    "# Let's generate 400 examples\n",
    "N = 400\n",
    "x = np.random.uniform(low=-75, high=100, size=N)\n",
    "y = 2*x\n",
    "X = np.zeros((N, 3))\n",
    "X[:,0] = x\n",
    "X[:,1] = x**2\n",
    "X[:,2] = x**3\n",
    "\n",
    "X_tensor = Variable(torch.FloatTensor(X), requires_grad=False)\n",
    "y_tensor = Variable(torch.FloatTensor(y), requires_grad=False)\n",
    "\n",
    "# Test set initialization\n",
    "X_test = np.zeros((3, 3))\n",
    "X_test[:,0] = np.array([-2.5, 0.0, 19])\n",
    "X_test[:,1] = X_test[:,0]**2\n",
    "X_test[:,2] = X_test[:,0]**3\n",
    "X_test_tsr = Variable(torch.FloatTensor(X_test), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple linear Neural Net which is y = w_1*x + w_2*x^2 + w_3*x^3 + b\n",
    "import math\n",
    "\n",
    "def RunLinearNNTraining(X, y, learning_rate=1e-5, epochs=5000, batch_size=None, X_test=None, use_optimizer=None):\n",
    "    \n",
    "    # Neural Net\n",
    "    X_size = X.size()\n",
    "    N = X_size[0]\n",
    "    D_in = X_size[1]\n",
    "    D_out = 1\n",
    "    model = torch.nn.Linear(D_in, D_out)\n",
    "    loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "    \n",
    "    # Choose Optimizer\n",
    "    optimizer = None\n",
    "    if use_optimizer:\n",
    "        if use_optimizer == 'SGD':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        elif use_optimizer == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        elif use_optimizer == 'Adadelta':\n",
    "            optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "        elif use_optimizer == 'ASGD':\n",
    "            optimizer = torch.optim.ASGD(model.parameters(), lr=learning_rate)\n",
    "        elif use_optimizer == 'RMSprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            print(\"Invalid Optimizer\")\n",
    "            use_optimizer=None\n",
    "    \n",
    "    losses = []\n",
    "    loss = None\n",
    "    for t in range(epochs):\n",
    "        num_batches = 1\n",
    "        X_batch = None\n",
    "        y_batch = None\n",
    "        if batch_size:\n",
    "            num_batches = math.ceil(N/batch_size)\n",
    "        else:\n",
    "            batch_size = N\n",
    "        \n",
    "        shuffle = torch.randperm(N)\n",
    "        \n",
    "        for b in range(num_batches):\n",
    "            lower_index = b*batch_size\n",
    "            upper_index = min(lower_index+batch_size, N)\n",
    "            indices = shuffle[lower_index:upper_index]\n",
    "            X_batch = X[indices]\n",
    "            y_batch = y[indices]\n",
    "                        \n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            losses.append(loss.data[0])\n",
    "            \n",
    "            if use_optimizer:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                # Zero the gradients before running the backward pass.\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights using gradient descent. Each parameter is a Variable, so\n",
    "                # we can access its data and gradients like we did before.\n",
    "                for param in model.parameters():\n",
    "                    param.data -= learning_rate * param.grad.data\n",
    "\n",
    "    print(\"Final Loss: \", loss.data[0])\n",
    "    print(\"Parameters [w_1, w_2, w_3, b]: \")\n",
    "    for param in model.parameters():\n",
    "        print(param.data[0])\n",
    "\n",
    "    # plot Loss vs Iterations\n",
    "    plt.plot(losses)\n",
    "    plt.title('Loss history')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # Predictions on Test set\n",
    "    if X_test:\n",
    "        print(\"Test:\")\n",
    "        print(\"X_test: \", X_test.data)\n",
    "        print(\"y_pred: \", model(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full batch gradient descent\n",
    "### SGD:\n",
    "It doesn't converge to global optima and the learning rate has to be set very low if not the gradients explode.\n",
    "### Adam and RMSprop:\n",
    "It needs a lot of epochs to converge close to global optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss:  0.014629274606704712\n",
      "Parameters [w_1, w_2, w_3, b]: \n",
      "\n",
      " 1.9988e+00\n",
      "-5.3303e-05\n",
      " 4.9020e-07\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "0.18852441012859344\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuYXXV97/H3Zy5JIBeSmAFCLiThIkQtF0eQai1tLQZO\na2y1CkdavDXWI9rap+2DtUd5sOfUy2l7tKIYNQVvAQFp0x4QqBZREcgEkUtsJIRAJoZkkgm5k8zM\n/p4/1m+Slcnee9Yks2aPyef1PPuZtX/rt9b67rXXrO9e67fW+ikiMDMzG0xTowMwM7NfDk4YZmZW\niBOGmZkV4oRhZmaFOGGYmVkhThhmZlaIE4bZMJP0Dkk/rDP+LklXjWRMZsPBCcOOWpLWSnp9o+MY\nKCIujYibBqsnKSSdPhIxmRXhhGF2FJLU0ugY7OjjhGHHJEl/LGm1pG5JyySdksol6R8lbZK0XdLj\nkl6exl0maaWkHZLWS/qLQZbxfyRtlfSMpEtz5fdJek8aPl3S9yVtk7RZ0i2p/P5U/aeSdkp6W724\n07iQ9H5JTwFPSbpe0t8PiGmZpA8d+Rq0Y5EThh1zJP0m8HfAW4HpwLPAzWn0JcDrgDOBE1KdLWnc\nV4D3RsRE4OXA9+os5kJgFTAN+BTwFUmqUu/jwD3AFGAm8E8AEfG6NP6ciJgQEbcMEne/N6Vlzwdu\nAq6Q1JQ+9zTg9cA368RtVtNRlzAkLUm/Dp8oUHe2pP+U9BNJj0m6bCRitIZ7O7AkIh6JiL3Ah4GL\nJM0BeoCJwFmAIuJnEbEhTdcDzJc0KSK2RsQjdZbxbER8KSL6yHbc04GTqtTrAU4FTomIFyOiZmP5\nIHH3+7uI6I6IPRHxMLAN+K007nLgvojYWGcZZjUddQkDuBFYULDu3wDfiojzyP6ZPl9WUDaqnEL2\n6xyAiNhJdhQxIyK+B3wOuB7YJGmxpEmp6puBy4Bn02mki+os4/nc/HenwQlV6v0VIOBhSU9Ketfh\nxJ2rs27ANDcBV6bhK4Gv1Zm/WV1HXcKIiPuB7nyZpNMkfUfSCkk/kHRWf3Wgf2dwAvCLEQzVGucX\nZL/qAZA0HngJsB4gIj4bEa8kO61zJvCXqXx5RCwETgT+BfjWkQYSEc9HxB9HxCnAe4HP17kyqm7c\n/bMcMM3XgYWSzgHOTnGbHZajLmHUsBj4QNoJ/AUHjiSuBa6U1AncCXygMeFZiVoljcu9WoClwDsl\nnStpLPC/gYciYq2kV0m6UFIrsAt4EahIGiPp7ZJOiIgeYDtQOdLgJP2BpJnp7VayHX7/fDcC83LV\na8Zda/4R0QksJzuyuD0i9hxpzHbsOuoThqQJwK8Ct0p6FPgi2flkgCuAGyNiJtmphq/1NxDaUeNO\nYE/udW1E/AfwP4HbgQ3AaWSnJCE74vwS2c77WbJTPp9O4/4QWCtpO/AnZG0KR+pVwEOSdgLLgD+N\niDVp3LXATZJekPTWQeKu5ybgFfh0lB0hHY0dKKVGwH+PiJen88+rImJ6lXpPAgsiYl16vwZ4dURs\nGsl4zcok6XVkp6ZOjaPxH95GzFH/azoitgPPSPoD2H+d/Tlp9HOkK0gknQ2MA7oaEqhZCdKptT8F\nvuxkYUfqqDvCkLQUuJjs+veNwMfIrpf/AtmpqFbg5oi4TtJ8stMPE8jOHf9VRNzTiLjNhlv6EdQB\n/JTsSHp7g0OyX3KlJQxJs4Cvkl17HsDiiPjMgDoCPkPWfrAbeEf/te3KHs72N6nq3xZ59o6ZmZWn\nzIQxHZgeEY9ImgisAN4UEStzdS4juzLpMrK7Uz8TERdKmkr2y6idLNmsAF4ZEVtLCdbMzAZV2gPK\n0t2xG9LwDkk/I7vBaGWu2kLgq+nc6oOSJqdEczFwb0R0A0i6l+xmvKX1ljlt2rSYM2fOcH8UM7Oj\n1ooVKzZHRFuRuiPyRMt01dJ5wEMDRs3g4DtTO1NZrfK65syZQ0dHx5GEamZ2TJH07OC1MqVfJZXu\ng7gd+LMyGt0kLZLUIamjq8sXOJmZlaXUhJEu6bsd+EZEfLtKlfXArNz7mamsVvkhImJxRLRHRHtb\nW6GjKjMzOwylJYx0BdRXgJ9FxD/UqLYM+KN0b8SrgW2p7eNu4BJJUyRNIXvk9N1lxWpmZoMrsw3j\nNWSPUng8PZID4K+B2QARcQPZYxsuA1aTXVb7zjSuW9LHyZ6BA3BdfwO4mZk1RplXSf2Q7LHN9eoE\n8P4a45YAS0oIzczMDsNR/2gQMzMbHk4YZmZWiBMG8NnvPsX3f+5Lcs3M6nHCAL5w39P8aPXmRodh\nZjaqOWGYmVkhThhmZlaIE4aZmRXihGFmZoU4YZiZWSFOGGZmVogThpmZFeKEYWZmhThhJGX1bW5m\ndrRwwgBU95m6ZmYGThhmZlaQE4aZmRXihGFmZoWU1uOepCXA7wCbIuLlVcb/JfD2XBxnA22pe9a1\nwA6gD+iNiPay4jQzs2LKPMK4EVhQa2REfDoizo2Ic4EPA98f0G/3b6TxThZmZqNAaQkjIu4Huget\nmLkCWFpWLGZmduQa3oYh6XiyI5Hbc8UB3CNphaRFg0y/SFKHpI6uLveaZ2ZWloYnDOB3gR8NOB31\n2og4H7gUeL+k19WaOCIWR0R7RLS3tbWVHauZ2TFrNCSMyxlwOioi1qe/m4A7gAvKDsI3epuZ1dfQ\nhCHpBODXgX/NlY2XNLF/GLgEeKLUOMqcuZnZUaLMy2qXAhcD0yR1Ah8DWgEi4oZU7feAeyJiV27S\nk4A7lD2vowX4ZkR8p6w4zcysmNISRkRcUaDOjWSX3+bL1gDnlBOVmZkdrtHQhmFmZr8EnDDMzKwQ\nJwwzMyvECcPMzApxwjAzs0KcMMzMrBAnjMQ3epuZ1eeEAcidepuZDcoJw8zMCnHCMDOzQpwwzMys\nECcMMzMrxAnDzMwKccIwM7NCnDDMzKwQJwwzMyvECSNxn95mZvWVljAkLZG0SVLV/rglXSxpm6RH\n0+ujuXELJK2StFrSNWXFuH95ZS/AzOwoUOYRxo3AgkHq/CAizk2v6wAkNQPXA5cC84ErJM0vMU4z\nMyugtIQREfcD3Ycx6QXA6ohYExH7gJuBhcManJmZDVmj2zAukvRTSXdJelkqmwGsy9XpTGVVSVok\nqUNSR1dXV5mxmpkd0xqZMB4BTo2Ic4B/Av7lcGYSEYsjoj0i2tva2oY1QDMzO6BhCSMitkfEzjR8\nJ9AqaRqwHpiVqzozlZmZWQM1LGFIOlmpIwpJF6RYtgDLgTMkzZU0BrgcWNaoOM3MLNNS1owlLQUu\nBqZJ6gQ+BrQCRMQNwFuA90nqBfYAl0dEAL2SrgbuBpqBJRHxZFlxmplZMaUljIi4YpDxnwM+V2Pc\nncCdZcRVMx530mpmVlejr5IaHXznnpnZoJwwzMysECcMMzMrxAnDzMwKccIwM7NCnDDMzKwQJwwz\nMyvECcPMzApxwjAzs0KcMBJ30WpmVp8TBr7R28ysCCcMMzMrxAnDzMwKccIwM7NCnDDMzKwQJwwz\nMyvECcPMzAopLWFIWiJpk6Qnaox/u6THJD0u6QFJ5+TGrU3lj0rqKCtGMzMrrswjjBuBBXXGPwP8\nekS8Avg4sHjA+N+IiHMjor2k+MzMbAjK7NP7fklz6ox/IPf2QWBmWbGYmdmRGy1tGO8G7sq9D+Ae\nSSskLao3oaRFkjokdXR1dR3WwiXf621mNpjSjjCKkvQbZAnjtbni10bEekknAvdK+q+IuL/a9BGx\nmHQ6q7293U+EMjMrSUOPMCT9CvBlYGFEbOkvj4j16e8m4A7ggsZEaGZm/RqWMCTNBr4N/GFE/DxX\nPl7SxP5h4BKg6pVWZmY2cko7JSVpKXAxME1SJ/AxoBUgIm4APgq8BPh8akPoTVdEnQTckcpagG9G\nxHfKitPMzIop8yqpKwYZ/x7gPVXK1wDnHDqFmZk10mi5SsrMzEY5JwwzMyvECcPMzApxwkjCnXqb\nmdXlhAH4Rm8zs8E5YZiZWSFOGGZmVogThpmZFeKEYWZmhThhmJlZIU4YZmZWiBOGmZkV4oRhZmaF\nOGEkvs/bzKw+JwzAN3qbmQ3OCcPMzAoplDAknSZpbBq+WNIHJU0uNzQzMxtNih5h3A70STodWAzM\nAr452ESSlkjaJKlqn9zKfFbSakmPSTo/N+4qSU+l11UF4zQzs5IUTRiViOgFfg/4p4j4S2B6gelu\nBBbUGX8pcEZ6LQK+ACBpKlkf4BcCFwAfkzSlYKxmZlaCogmjR9IVwFXAv6ey1sEmioj7ge46VRYC\nX43Mg8BkSdOBNwD3RkR3RGwF7qV+4jEzs5IVTRjvBC4C/ldEPCNpLvC1YVj+DGBd7n1nKqtVfghJ\niyR1SOro6uoahpDMzKyaliKVImIl8EGAdGpoYkR8sszAioqIxWTtKrS3t/t2CjOzkhS9Suo+SZNS\n28IjwJck/cMwLH89WQN6v5mprFa5mZk1SNFTUidExHbg98naHC4EXj8My18G/FG6WurVwLaI2ADc\nDVwiaUo6orkklZXGXXqbmdVX6JQU0JIao98KfKTozCUtBS4GpknqJLvyqRUgIm4A7gQuA1YDu8na\nSoiIbkkfB5anWV0XEfUaz4+I3Km3mdmgiiaM68h+4f8oIpZLmgc8NdhEEXHFIOMDeH+NcUuAJQXj\nMzOzkhVt9L4VuDX3fg3w5rKCMjOz0adoo/dMSXeku7Y3Sbpd0syygzMzs9GjaKP3P5M1UJ+SXv+W\nyszM7BhRNGG0RcQ/R0Rvet0ItJUYl5mZjTJFE8YWSVdKak6vK4EtZQZmZmajS9GE8S6yS2qfBzYA\nbwHeUVJMZmY2ChVKGBHxbES8MSLaIuLEiHgTvkrKzOyYciQ97v35sEUxCoR79TYzq+tIEsZRc3v0\nUfNBzMxKdCQJwz/JzcyOIXXv9Ja0g+qJQcBxpURkZmajUt2EERETRyoQMzMb3Y7klJSZmR1DnDDM\nzKwQJwwzMyvECcPMzAopNWFIWiBplaTVkq6pMv4fJT2aXj+X9EJuXF9u3LIy4zQzs8EV7XFvyCQ1\nA9cDvw10AsslLYuIlf11IuJDufofAM7LzWJPRJxbVnwDuU9vM7P6yjzCuABYHRFrImIfcDOwsE79\nK4ClJcZTk7v0NjMbXJkJYwawLve+M5UdQtKpwFzge7nicZI6JD0o6U3lhWlmZkWUdkpqiC4HbouI\nvlzZqRGxXtI84HuSHo+IpwdOKGkRsAhg9uzZIxOtmdkxqMwjjPXArNz7mamsmssZcDoqItanv2uA\n+zi4fSNfb3FEtEdEe1ubOwE0MytLmQljOXCGpLmSxpAlhUOudpJ0FjAF+HGubIqksWl4GvAaYOXA\nac3MbOSUdkoqInolXQ3cDTQDSyLiSUnXAR0R0Z88LgdujjjoOqWzgS9KqpAltU/kr64yM7ORV2ob\nRkTcCdw5oOyjA95fW2W6B4BXlBmbmZkNje/0NjOzQpwwzMysECeMxDd6m5nV54QBuFdvM7PBOWGY\nmVkhThhmZlaIE4aZmRXihGFmZoU4YZiZWSFOGGZmVogThpmZFeKEYWZmhThhJO7T28ysPicM3Ke3\nmVkRThhmZlaIE4aZmRXihGFmZoWUmjAkLZC0StJqSddUGf8OSV2SHk2v9+TGXSXpqfS6qsw4zcxs\ncKV10SqpGbge+G2gE1guaVmVvrlviYirB0w7FfgY0E7WVcWKNO3WsuI1M7P6yjzCuABYHRFrImIf\ncDOwsOC0bwDujYjulCTuBRaUFKeZmRVQZsKYAazLve9MZQO9WdJjkm6TNGuI0yJpkaQOSR1dXV3D\nEbeZmVXR6EbvfwPmRMSvkB1F3DTUGUTE4ohoj4j2tra2IwjFd+6ZmdVTZsJYD8zKvZ+ZyvaLiC0R\nsTe9/TLwyqLTDifft2dmNrgyE8Zy4AxJcyWNAS4HluUrSJqee/tG4Gdp+G7gEklTJE0BLkllZmbW\nIKVdJRURvZKuJtvRNwNLIuJJSdcBHRGxDPigpDcCvUA38I40bbekj5MlHYDrIqK7rFjNzGxwpSUM\ngIi4E7hzQNlHc8MfBj5cY9olwJIy4zMzs+Ia3ehtZma/JJwwzMysECcMMzMrxAnDzMwKccIwM7NC\nnDASd9FqZlafEwbuotXMrAgnDDMzK8QJA9i4fS8PPL2l0WGYmY1qThjJc927Gx2Cmdmo5oRhZmaF\nOGGYmVkhThhmZlaIE4aZmRXihGFmZoU4YZiZWSFOGGZmVkipCUPSAkmrJK2WdE2V8X8uaaWkxyR9\nV9KpuXF9kh5Nr2UDpzUzs5FVWhetkpqB64HfBjqB5ZKWRcTKXLWfAO0RsVvS+4BPAW9L4/ZExLll\nxWdmZkNT5hHGBcDqiFgTEfuAm4GF+QoR8Z8R0X+L9YPAzBLjMTOzI1BmwpgBrMu970xltbwbuCv3\nfpykDkkPSnpTrYkkLUr1Orq6uo4sYjMzq6m0U1JDIelKoB349VzxqRGxXtI84HuSHo+IpwdOGxGL\ngcUA7e3t7tXCzKwkZR5hrAdm5d7PTGUHkfR64CPAGyNib395RKxPf9cA9wHnlRirmZkNosyEsRw4\nQ9JcSWOAy4GDrnaSdB7wRbJksSlXPkXS2DQ8DXgNkG8sNzOzEVbaKamI6JV0NXA30AwsiYgnJV0H\ndETEMuDTwATgVmXd3j0XEW8Ezga+KKlCltQ+MeDqKjMzG2GltmFExJ3AnQPKPpobfn2N6R4AXlFm\nbGZmNjS+09vMzApxwjAzs0KcMMzMrBAnDDMzK8QJw8zMCnHCMDOzQpwwzMysECcMMzMrxAnDzMwK\nccIwM7NCnDDMzKwQJwwzMyvECcPMzApxwjAzs0KcMMzMrBAnDDMzK8QJI2fPvr5Gh2BmNmqVmjAk\nLZC0StJqSddUGT9W0i1p/EOS5uTGfTiVr5L0hjLj7Lf+hT0jsRgzs19KpSUMSc3A9cClwHzgCknz\nB1R7N7A1Ik4H/hH4ZJp2PnA58DJgAfD5NL9SvbB7X9mL+KX03JbdVct/+NRm+ipRddxTG3cQcei4\nX7ywh+5dh67n3r4Ka7p2Vp3XM5t3VZ3X0107ay5jb++hR4udW3eze1/vIeWbdrzItt09h5Rvf7GH\nrVVi3bprX9Vt5YXd+9iw7dAfHZVKsHbzrkPKI4In1m87pLx/GdWWvePFHlY9v6PqNJt2vMiWnXsP\nKe+rBOu6D/0Oe/oqdO04tH73rn1sf/HQ9bF5596q62/t5l3sqFJ/b29f1eX2VYLnt714SPnOvb1s\n2nFo+fYXe6puMy/s3lc1ns6tu6kM2C4jgme3VP8OOtZ2V92Odu3t5eka2+S67t1VYwJ4vHNb1fl1\n7dhbdTvbtqeH1Zuqf6frundXPfOxZedeNlf5rstWZp/eFwCrI2INgKSbgYXAylydhcC1afg24HOS\nlMpvjoi9wDOSVqf5/bjEeHnLDT9m3rTxoIPLVb06UeNNvjy/4Rxc3l8Wh5QNHD5oMYPMr9Y8a9Wl\n1vLT3/w/xRknTtg/bvWmA/9Ip6fyfgPHVSLo7Qt6+yr8Iu0o8tNUKsGatEOdOK6FkyaN2z9u7eZd\n9KZ//tlTj6enr8K+3gpbcnGd+pLj6e0Levoq9PRV2Jr+KWdPPZ69vX3s662wt7fC7vSPl192XyV4\nJi17Xtt4IqASQSWCdd3Zzv+0tvH0VYK+CPr6Yv9nOC3V7xtQv3/+lQj29Vbo3HogicybNp596TNs\nyu2sTz9xAhHBvr4Ke3sOjDvtoJjgubQDnjX1OJqlbNmVtBPensU1d9p49vb08WJvhb09fexKn/vk\nSeOYOK6FvvR99M9r7rTxRJp/XyX2H2mffuIEKpVgb2+Ffbnkkv9Oe/oqbEjrY17beHr6KvvLN+/c\nt3/+lQh6eivs64v9O7oZk4/juDHZ78Devgpr0w+T00+cQF8l6K1k89qQ22b6ywd+D72VbP4btr+4\nfzueN208PQPqwoHtuBLB010HksiclxxPT1/2HfT0VXghbUdzp41HytZNb18ctK7nTcuW3R/Xxu0H\nvtP+z91XCSIOnMGY1zaeSqX6+o6I/Z8lv65mTT2O1uYmevuCvb19+5cze+rxtDSLaRPG8q33XkTZ\nykwYM4B1ufedwIW16kREr6RtwEtS+YMDpp1RbSGSFgGLAGbPnn1YgX793Rdy5Vce4tXzpjJtwtiD\nxh2y3w4OyiD5ZJLlumrl9esflJAOqpubX815FKivAzWGOr9de3v59k/WM35MM2ecdGBH+7JTJvGv\nj/6CJsFLT5qY/wS89OSJ/L/HNjCutSkbJ2htEi3NTdy2ojOrk59GMG3CWB5e283Z0yfRlvsOTmsb\nz91PbqS5SZw/ezJjWppobW6ipUnc9ONnaW0W586aTEtTE63NoqVZfP3B5wA4f/ZkxrY0M7a1iTHN\nTXz5h88csuymJu1PGPOnT6JJoknQJLGuez0AZ508ieYm0dIkmpq0/zOclavfnKt/5kkTsu9CMLal\niVlTXuTHa7Zw/uzJzJhyPK3NYkxzE32V4NYVnVwwZyptE7PPPKalibEtTdy8fN3+ZTc1ZcsQ0NIs\n1nTtYv70SYxtaaa5STRJNDfBtzqyuF4+4wTGtTQxtrWJsS3NbN/Tw60rOjn9xAlMOq4FIca0NDFh\nbAsrN2znrJMn0tLcRLOy9fHtR9bvX09Sf0zNLH34uQPrTzAmfQ+3pvVx9vRJ+8taW5r4+fM76Hh2\nK6efOIHxY5ppbW6itaWJLTv3cveTGzntxAlMHNuy/3voTxhnnjSB5qZsPvn595e3NonmXPlZJ0+i\npVn7t4v+dfeyGSfsn8e2PT3cs3Ij586azCmTx+3fvs+ePol/f2wD586azOypx9Pa3MSYlmxeq57f\nwUPPdHNa23jGtTbv//5bmrR/Xc8/ZRItTcriaha9leC2FZ289KSJnHnyxGyd6uDtpn87a24SUnak\nvmnHXk5vm5BtZ+mztDY3sfTh5xjX2kT7qVPZ11ehtUmMbWnmu/+1kc0793H+7Mn0VoIJY8vclR+g\naodOwzJj6S3Agoh4T3r/h8CFEXF1rs4TqU5nev80WVK5FngwIr6eyr8C3BURt9VbZnt7e3R0dJTx\ncczMjkqSVkREe5G6ZTZ6rwdm5d7PTGVV60hqAU4AthSc1szMRlCZCWM5cIakuZLGkDViLxtQZxlw\nVRp+C/C9yA55lgGXp6uo5gJnAA+XGKuZmQ2itBNfqU3iauBuoBlYEhFPSroO6IiIZcBXgK+lRu1u\nsqRCqvctsgbyXuD9EeGbJMzMGqi0NoxGcBuGmdnQjJY2DDMzO4o4YZiZWSFOGGZmVogThpmZFXJU\nNXpL6gKePczJpwGbhzGc4eK4hsZxDY3jGpqjMa5TI6KtSMWjKmEcCUkdRa8UGEmOa2gc19A4rqE5\n1uPyKSkzMyvECcPMzApxwjhgcaMDqMFxDY3jGhrHNTTHdFxuwzAzs0J8hGFmZoU4YZiZWSHHfMKQ\ntEDSKkmrJV0zAsubJek/Ja2U9KSkP03l10paL+nR9LosN82HU3yrJL2hrNglrZX0eFp+RyqbKule\nSU+lv1NSuSR9Ni37MUnn5+ZzVar/lKSrai2vYEwvza2TRyVtl/RnjVhfkpZI2pQ6/uovG7b1I+mV\naf2vTtPW6h24SFyflvRfadl3SJqcyudI2pNbbzcMtvxan/Ew4xq2701Z1wkPpfJblHWjcLhx3ZKL\naa2kRxuwvmrtGxq+je0XEcfsi+yx608D84AxwE+B+SUvczpwfhqeCPwcmE/Wy+BfVKk/P8U1Fpib\n4m0uI3ZgLTBtQNmngGvS8DXAJ9PwZcBdZD2Hvhp4KJVPBdakv1PS8JRh/L6eB05txPoCXgecDzxR\nxvoh6/Pl1Wmau4BLjyCuS4CWNPzJXFxz8vUGzKfq8mt9xsOMa9i+N+BbwOVp+AbgfYcb14Dxfw98\ntAHrq9a+oeHbWP/rWD/CuABYHRFrImIfcDOwsMwFRsSGiHgkDe8AfkaN/sqThcDNEbE3Ip4BVqe4\nRyr2hcBNafgm4E258q9G5kFgsqTpwBuAeyOiOyK2AvcCC4Yplt8Cno6Ienfzl7a+IuJ+sn5bBi7v\niNdPGjcpIh6M7D/7q7l5DTmuiLgnInrT2wfJeq2saZDl1/qMQ46rjiF9b+mX8W8C/d02D0tcab5v\nBZbWm0dJ66vWvqHh21i/Yz1hzADW5d53Un/nPawkzQHOAx5KRVenQ8slucPYWjGWEXsA90haIWlR\nKjspIjak4eeBkxoQV7/LOfgfudHrC4Zv/cxIw8MdH8C7yH5N9psr6SeSvi/p13Lx1lp+rc94uIbj\ne3sJ8EIuKQ7X+vo1YGNEPJUrG/H1NWDfMGq2sWM9YTSMpAnA7cCfRcR24AvAacC5wAayw+KR9tqI\nOB+4FHi/pNflR6ZfJQ25Djudn34jcGsqGg3r6yCNXD+1SPoIWa+V30hFG4DZEXEe8OfANyVNKjq/\nYfiMo+57G+AKDv5RMuLrq8q+4YjmN5yO9YSxHpiVez8zlZVKUivZBvGNiPg2QERsjIi+iKgAXyI7\nFK8X47DHHhHr099NwB0pho3pULb/MHzTSMeVXAo8EhEbU4wNX1/JcK2f9Rx82uiI45P0DuB3gLen\nHQ3plM+WNLyCrH3gzEGWX+szDtkwfm9byE7BtAwoP2xpXr8P3JKLd0TXV7V9Q535jfw2NpQGj6Pt\nRdan+RqyRrb+BrWXlbxMkZ07/L8Dyqfnhj9Edj4X4GUc3Bi4hqwhcFhjB8YDE3PDD5C1PXyagxvc\nPpWG/xsHN7g9nMqnAs+QNbZNScNTh2G93Qy8s9HriwGNoMO5fji0QfKyI4hrAbASaBtQrw1oTsPz\nyHYYdZdf6zMeZlzD9r2RHW3mG73/x+HGlVtn32/U+qL2vmFUbGMRcWwnjLQCLyO7GuFp4CMjsLzX\nkh1SPgY8ml6XAV8DHk/lywb8Y30kxbeK3FUNwxl7+mf4aXo92T8/snPF3wWeAv4jt+EJuD4t+3Gg\nPTevd5E1Wq4mt5M/gtjGk/2iPCFXNuLri+xUxQagh+z877uHc/0A7cATaZrPkZ7EcJhxrSY7j92/\njd2Q6r58xwmAAAACSElEQVQ5fb+PAo8AvzvY8mt9xsOMa9i+t7TNPpw+663A2MONK5XfCPzJgLoj\nub5q7Rsavo31v/xoEDMzK+RYb8MwM7OCnDDMzKwQJwwzMyvECcPMzApxwjAzs0KcMMyqkLQz/Z0j\n6b8P87z/esD7B4Zz/mZlccIwq28OMKSEkbv7uJaDEkZE/OoQYzJrCCcMs/o+Afxa6gvhQ5KalfU1\nsTw9QO+9AJIulvQDScvI7rBG0r+kBzk+2f8wR0mfAI5L8/tGKus/mlGa9xOpz4K35eZ9n6TblPVx\n8Y0h92NgNgwG+yVkdqy7hqz/ht8BSDv+bRHxKkljgR9JuifVPR94eWSP5wZ4V0R0SzoOWC7p9oi4\nRtLVEXFulWX9PtlD+c4BpqVp7k/jziN7fMYvgB8BrwF+OPwf16w2H2GYDc0lwB8p65HtIbLHNpyR\nxj2cSxYAH5T0U7L+KGbl6tXyWmBpZA/n2wh8H3hVbt6dkT2071GyU2VmI8pHGGZDI+ADEXH3QYXS\nxcCuAe9fD1wUEbsl3QeMO4Ll7s0N9+H/XWsAH2GY1beDrLvMfncD70uPoUbSmZLGV5nuBGBrShZn\nkT0htF9P//QD/AB4W2onaSPrSvThYfkUZsPAv1LM6nsM6Eunlm4EPkN2OuiR1PDcRfVuLr8D/Imk\nn5E9ffXB3LjFwGOSHomIt+fK7wAuInticAB/FRHPp4Rj1nB+Wq2ZmRXiU1JmZlaIE4aZmRXihGFm\nZoU4YZiZWSFOGGZmVogThpmZFeKEYWZmhfx/hs6hzpZ3YYwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113c16f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\n",
      "X_test:  \n",
      "   -2.5000     6.2500   -15.6250\n",
      "    0.0000     0.0000     0.0000\n",
      "   19.0000   361.0000  6859.0000\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "y_pred:  Variable containing:\n",
      " -4.8088\n",
      "  0.1885\n",
      " 38.1494\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use_optimizer can be Adam, RMSprop, Adadelta, ASGD, SGD\n",
    "RunLinearNNTraining(X=X_tensor, y=y_tensor, batch_size=None, epochs=20000, learning_rate=1e-2, \n",
    "                    X_test=X_test_tsr, use_optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full batch gradient descent with feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2599.5002   109.1683     4.5167\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "Variable containing:\n",
      " 15689.0000    624.9900     29.0000\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "Variable containing:\n",
      "-0.1657 -0.1747 -0.1523\n",
      "-0.1656 -0.1731 -0.1213\n",
      "-0.1652 -0.1683 -0.0868\n",
      "-0.1640 -0.1603 -0.0523\n",
      "-0.1698 -0.1491 -0.2937\n",
      " 0.8302  0.8253  0.7063\n",
      "[torch.FloatTensor of size 6x3]\n",
      "\n",
      "Variable containing:\n",
      " -0.3033\n",
      " -0.3688\n",
      " -1.1180\n",
      " -2.4675\n",
      " -9.6742\n",
      " 50.0109\n",
      "[torch.FloatTensor of size 6x1]\n",
      "\n",
      "Final Loss:  17.759979248046875\n",
      "\n",
      " 0.0193 -0.4158  0.3632\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "\n",
      "-0.3355\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAEWCAYAAADPZygPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZK6MYQkSd38f9V7nDzznSblAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1148902b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's try to find the equation y = 2 * x\n",
    "# We have 7 examples:- (x,y) = (0.1,0.2), (1,2), (2, 4), (3, 6), (-4, -8), (2.5, 5.0), (0, 0), (-2.5, -5.0)\n",
    "N = 6\n",
    "D_in = 1\n",
    "D_out = 1\n",
    "X_tensor = Variable(torch.FloatTensor(x), requires_grad=False)\n",
    "x = Variable(torch.FloatTensor([0.1, 1, 2, 3, -4, 25]), requires_grad=False).view(N,D_in)\n",
    "y = Variable(torch.FloatTensor([0.2, 2, 4, 6, -8, 50]), requires_grad=False).view(N,D_in)\n",
    "x_test = torch.FloatTensor([2.5, 0, -2.5]).view(-1,1)\n",
    "y_test = torch.FloatTensor([5, 0, -5]).view(-1,1)\n",
    "\n",
    "x_3 = torch.pow(x,3)\n",
    "x_2 = torch.pow(x,2)\n",
    "X = torch.cat([x_3, x_2, x], 1)\n",
    "X_min = torch.min(X,0)\n",
    "X_max = torch.max(X,0)\n",
    "X_mean = torch.mean(X,0)\n",
    "X_sub_mean = X-X_mean.expand_as(X)\n",
    "X_max_sub_min = X_max[0]-X_min[0] + 1e-7\n",
    "print(X_mean)\n",
    "print(X_max_sub_min)\n",
    "X_transformed = X_sub_mean/X_max_sub_min.expand_as(X_sub_mean)\n",
    "print(X_transformed)\n",
    "D_in = 3\n",
    "\n",
    "# Let's assume, y = w_1*x^3 + w_2*x^2 + w_3*x + b\n",
    "model = torch.nn.Linear(D_in, D_out)\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "l1 = torch.nn.L1Loss(size_average=True)\n",
    "learning_rate = 1e-8\n",
    "regularization_strength = 0.01\n",
    "losses = []\n",
    "y_pred = None\n",
    "loss = None\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(100000):\n",
    "    y_pred = model(X)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    reg_loss = 0\n",
    "    for param in model.parameters():\n",
    "        size= param.size()\n",
    "        reg_loss += l1(param, Variable(torch.zeros(size), requires_grad=False))\n",
    "    #loss += regularization_strength * reg_loss\n",
    "    #print(t, loss.data[0])\n",
    "    losses.append(loss.data[0])\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "    #optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Variable, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    #optimizer.step()\n",
    "    for param in model.parameters():\n",
    "        #print(param.grad.data)\n",
    "        param.data -= learning_rate * param.grad.data\n",
    "\n",
    "print(y_pred)\n",
    "print(\"Final Loss: \", loss.data[0])\n",
    "for param in model.parameters():\n",
    "    print(param.data)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# w = [0 0 58], b = 9.0334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = 0.1\n",
    "lr = 1e-5\n",
    "w = Variable(torch.randn(3), requires_grad=True)\n",
    "b = Variable(torch.zeros(1), requires_grad=True)\n",
    "x_3 = torch.pow(x,3)\n",
    "x_2 = torch.pow(x,2)\n",
    "X = torch.cat([x_3, x_2, x], 1)\n",
    "print(X)\n",
    "losses = []\n",
    "# Use autograd to compute the backward pass.\n",
    "for i in range(10000):\n",
    "    WX = X.mm(w.view(3,1))\n",
    "    output = WX + b.expand_as(WX)\n",
    "    loss = (y-output).pow(2).mean().sqrt() + torch.abs(w).sum()\n",
    "    losses.append(loss.data[0])\n",
    "    #print(\"Iteration \", i, \" : \", loss.data)\n",
    "    \n",
    "    loss.backward(retain_variables=True)\n",
    "    # Update weights using gradient descent\n",
    "    #print(w)\n",
    "    #print(output)\n",
    "    #print(w.grad.data)\n",
    "    #print(b.grad.data)\n",
    "    w.data -= lr * w.grad.data\n",
    "    b.data -= lr * b.grad.data\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    \n",
    "print(w)\n",
    "print(b)\n",
    "WX = X.mm(w.view(3,1))\n",
    "output = WX + b.expand_as(WX)\n",
    "loss = (y-output).pow(2).mean().sqrt() + torch.abs(w).sum()\n",
    "print(output)\n",
    "print(loss)\n",
    "plt.plot(losses)\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
